name: "debug"
description: "Classic deep policy"
device: cuda
cores: 4
seed: 0

environment:
  name: "ALE/Kangaroo-v5"
  object_centric: True
  reward_mode: env  # env, human, mixed (only relevant for scobi)
  framestack: 4
  frameskip: 4
  normalize_observation: True
  normalize_reward: False
  hack: 
    # modifs: ["disable_monkeys", "disable_coconut"]
    rewardfunc_path: "./in/reward_funcs/kangaroo.py"

general:
  hierarchy_shape: []  # e.g., [2, 4, 8] means 2 high-level, 4 mid-level, and 8 low-level options
  net_arch: [64, 64]  # the network architecture of each individual policy and value function
  total_timesteps: 10e6
  n_steps: 128
  n_epochs: 4
  batch_size: 256
  gamma: 0.99
  gae_lambda: 0.95
  normalize_advantage: True

meta_policy:
  logic: False
  policy_ent_coef: 0.01
  policy_clip_range:
     initial_value: 0.1
     schedule_type: linear
  value_fn_coef: 0.5
  value_fn_clip_range:
  learning_rate: 
    initial_value: 2.5e-4
    schedule_type: linear
  max_grad_norm: 0.5

evaluation:
  frequency: 100000
  n_episodes: 10
  deterministic: True
  render: False
  # early_stop_on_no_reward: 1000