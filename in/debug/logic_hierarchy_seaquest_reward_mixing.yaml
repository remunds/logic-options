name: "logic_hierarchy_reward_mixing_hp_change2"
description: ""
device: cuda:15
cores: 1
seed: 0

environment:
  name: "ALE/Seaquest-v5"
  object_centric: True
  reward_mode: env  # env, human, mixed
  framestack: 1
  frameskip: 1
  normalize_observation: False
  normalize_reward: False
  exclude_properties: False  # exclude object properties (like position) from feature vector
  freeze_invisible_obj: False
  hack:
    # modifs: 
    rewardfunc_path: ["./in/reward_funcs/seaquest/hackatari_reward.py",
    "./in/reward_funcs/seaquest/fight_enemies.py",
    "./in/reward_funcs/seaquest/collect_divers.py",
    "./in/reward_funcs/seaquest/surface.py"]

general:
  hierarchy_shape: [3]  # e.g., [2, 4, 8] means 2 high-level, 4 mid-level, and 8 low-level options
  net_arch: [64, 64]  # the network architecture of each individual policy and value function
  total_timesteps: 10e6
  n_steps: 128
  n_epochs: 4
  batch_size: 256  # decrease if out-of-memory error is observed
  gamma: 0.99
  gae_lambda: 0.95
  normalize_advantage: True

meta_policy:
  logic: True
  policy_ent_coef: 0.00001
  policy_clip_range:  # PPO clipping epsilon
    initial_value: 0.1
    schedule_type: linear
  value_fn_coef: 0.0025
  value_fn_clip_range: 
  learning_rate:  # Adam
    initial_value: 0.0005 
    schedule_type: linear
  max_grad_norm: 0.5
  policy_terminator: True
  policy_termination_mode: "raban"

options:
  policy_ent_coef: 0.01
  policy_clip_range:  # PPO clipping epsilon
    initial_value: 0.1
    schedule_type: linear
  value_fn_coef: 0.5
  value_fn_clip_range:
  terminator_ent_coef: 0.1
  terminator_clip_range: 0.1
  termination_regularizer: -100 # directly terminate for better learning
  # termination_regularizer: 0.01 #bacon et al.
  learning_rate:  # Adam
    initial_value: 2.5e-4
    schedule_type: linear
  max_grad_norm: 0.5

evaluation:
  frequency: 10000
  deterministic: True
  n_episodes: 10
  render: False